{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from math import ceil, sqrt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# colors generator\n",
    "from itertools import cycle\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# # Algorithms\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import pairwise_distances_argmin_min\n",
    "# from sklearn.decomposition import PCA\n",
    "# from tslearn.barycenters import dtw_barycenter_averaging\n",
    "# Modeling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# from statsmodels.tsa.arima_model import ARIMA\n",
    "# import pmdarima as pm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "# assert reproducibility\n",
    "import random\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_in_year = 12\n",
    "month_quarter =4\n",
    "window1 = 3\n",
    "window2 = 6\n",
    "window3 = 12\n",
    "\n",
    "def add_vars(y):\n",
    "    df = y.copy()\n",
    "    y_name = df.columns[0]\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month_sin'] = np.sin(2*np.pi*df['month']/month_in_year)\n",
    "    df['month_cos'] = np.cos(2*np.pi*df['month']/month_in_year)\n",
    "    df['quarter_sin'] = np.sin(2*np.pi*df['quarter']/month_quarter)\n",
    "    df['quarter_cos'] = np.cos(2*np.pi*df['quarter']/month_quarter)\n",
    "    \n",
    "    for i in range(1, 13):\n",
    "        df[f\"lag{i}\"] = df[y_name].shift(i)\n",
    "    df[\"Date\"] = y[[y_name]].index\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_rolled_3d = df[[y_name]].rolling(window=window1, min_periods=0)\n",
    "    df_rolled_7d = df[[y_name]].rolling(window=window2, min_periods=0)\n",
    "    df_rolled_30d = df[[y_name]].rolling(window=window3, min_periods=0)\n",
    "\n",
    "    df_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\n",
    "    df_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\n",
    "    df_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n",
    "\n",
    "    df_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\n",
    "    df_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\n",
    "    df_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n",
    "\n",
    "    df[f\"{y_name}_mean_lag{window1}\"] = df_mean_3d[y_name]\n",
    "    df[f\"{y_name}_mean_lag{window2}\"] = df_mean_7d[y_name]\n",
    "    df[f\"{y_name}_mean_lag{window3}\"] = df_mean_30d[y_name]\n",
    "\n",
    "    df[f\"{y_name}_std_lag{window1}\"] = df_std_3d[y_name]\n",
    "    df[f\"{y_name}_std_lag{window2}\"] = df_std_7d[y_name]\n",
    "    df[f\"{y_name}_std_lag{window3}\"] = df_std_30d[y_name]\n",
    "\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    df.set_index(\"Date\", drop=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_serie(df):\n",
    "    \"\"\"\n",
    "        Add vars & return X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    ts = df.copy()\n",
    "    ts = add_vars(ts)\n",
    "    y_name = ts.columns[0]\n",
    "    y = ts.dropna()[y_name]\n",
    "    X = ts.dropna().drop([y_name, 'Date'], axis=1)\n",
    "    \n",
    "    return timeseries_train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "        Add vars & return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_serie(df)\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "    X_train_scaled.columns = X_train.columns\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "    X_test_scaled.columns = X_train.columns\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# for time-series cross-validation set 5 folds \n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelResults(models, X_train, X_test, y_train, y_test,\\\n",
    "                     plot_intervals=False, plot_anomalies=False, axe = False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "        models = [[\"ridge\": mod_r], [\"lasso\": mod_l]]\n",
    "    \"\"\"\n",
    "    \n",
    "    errors = [0]*len(models)\n",
    "    predictions = [0]*len(models)\n",
    "    for i, model in enumerate(models) :\n",
    "        predictions[i] = model[1].predict(X_test)\n",
    "        errors[i] = mean_absolute_percentage_error(predictions[i], y_test)\n",
    "\n",
    "    min_index = errors.index(min(errors))\n",
    "    error = errors[min_index]\n",
    "    prediction = predictions[min_index]\n",
    "    winner = models[min_index][0]\n",
    "    model = models[min_index][1]\n",
    "    \n",
    "    index_of_pred = y_test.index\n",
    "    pred_series = pd.Series(prediction, index=index_of_pred)\n",
    "    \n",
    "    if not axe : \n",
    "        axe = plt\n",
    "        axe.figure(figsize=(15, 7))\n",
    "    axe.plot(pred_series, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    axe.plot(pd.concat([y_train, y_test]), label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = pred_series - (mae + scale * deviation)\n",
    "        upper = pred_series + (mae + scale * deviation)\n",
    "        \n",
    "        axe.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        axe.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            anomalies_series = pd.Series(anomalies, index=index_of_pred)\n",
    "            if ~np.isnan(anomalies).all() : axe.plot(anomalies_series, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    f = axe.title if axe == plt else axe.set_title\n",
    "    f(f\"{y_test.name},  model : {winner},  Mean absolute percentage error {error:.2f}%\")\n",
    "    axe.axvspan(index_of_pred[0], index_of_pred[-1],  color=sns.xkcd_rgb['grey'], alpha=0.2)\n",
    "    axe.legend(loc=\"best\")\n",
    "    \n",
    "def plotCoefficients(model, X_train):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### les fct aprés xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c5e3f4689fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Instantiate the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "def objective(space):\n",
    "    # Instantiate the classifier\n",
    "    clf = XGBRegressor(n_estimators =1000,colsample_bytree=space['colsample_bytree'],\n",
    "                           learning_rate = .3,\n",
    "                            max_depth = int(space['max_depth']),\n",
    "                            min_child_weight = space['min_child_weight'],\n",
    "                            subsample = space['subsample'],\n",
    "                           gamma = space['gamma'],\n",
    "                           reg_lambda = space['reg_lambda'])\n",
    "    \n",
    "    eval_set  = [(X_train_scaled, y_train), (X_test_scaled, y_test)]\n",
    "    \n",
    "    # Fit the classsifier\n",
    "    clf.fit(X_train_scaled, y_train,\n",
    "            eval_set=eval_set, eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "    # Predict on Cross Validation data\n",
    "    pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate our Metric - accuracy\n",
    "    accuracy = accuracy_score(y_test, pred>0.5)\n",
    "# return needs to be in this below format. We use negative of accuracy since we want to maximize it.\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space ={'max_depth': hp.quniform(\"max_depth\", 4, 16, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.7, 1),\n",
    "        'gamma' : hp.uniform ('gamma', 0.1,0.5),\n",
    "        'colsample_bytree' : hp.uniform ('colsample_bytree', 0.7,1),\n",
    "        'reg_lambda' : hp.uniform ('reg_lambda', 0,1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_features_forecast(series, models, test_size=0.15):\n",
    "    fig, axs = plt.subplots(5, 2, figsize=(20, 20))\n",
    "    axs = axs.flatten()\n",
    "    for i, col in enumerate(series.columns):\n",
    "        global X_train_scaled, y_train, X_test_scaled, y_test\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test = prepare_data(series[[col]])\n",
    "        for j in range(len(models)) :\n",
    "            if \"xgboost\" in models[j][0] :\n",
    "                if \"tune\" in models[j][0] :\n",
    "                    trials = Trials()\n",
    "                    best = fmin(fn=objective,\n",
    "                                space=space,\n",
    "                                algo=tpe.suggest,\n",
    "                                max_evals=10,\n",
    "                                trials=trials)                \n",
    "                    best['max_depth'] = int(best['max_depth'])\n",
    "\n",
    "                    models[j][1] = XGBRegressor(n_estimators =1000, **best)\n",
    "                    models[j][1].fit(X_train_scaled, y_train, \\\n",
    "                              eval_set=[(X_train_scaled, y_train),(X_test_scaled, y_test)],\\\n",
    "                              early_stopping_rounds=50, verbose=False)\n",
    "                else :\n",
    "                    models[j][1].fit(X_train_scaled, y_train, \\\n",
    "                              eval_set=[(X_train_scaled, y_train),(X_test_scaled, y_test)],\\\n",
    "                              early_stopping_rounds=50, verbose=False)\n",
    "            else : \n",
    "                models[j][1].fit(X_train_scaled, y_train)\n",
    "        plotModelResults(models, \n",
    "                         X_train=X_train_scaled, \n",
    "                         X_test=X_test_scaled, \n",
    "                         y_train=y_train,\n",
    "                         y_test=y_test,\n",
    "                         plot_intervals=True, plot_anomalies=True, axe = axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sales_s = pd.read_pickle(\"sales_s.pkl\")\n",
    "sales_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la démonstration de la demarche suivie dans l'apprentissage supervisé, on a selectionné aléatoirement un produit (P_64).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sales_s_p64 = sales_s[['P_64']]\n",
    "sales_s_p64.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On a commencé par implémenter le concept de \"feature engineering\" ou on a généré les variables nécessaires pour entrainer les modèles d'apprentissage.\n",
    "\n",
    "- Variables de temps : année, mois, trimestre\n",
    "- Valeurs retardées des séries temporelles\n",
    "- Statistiques : la moyenne et l'écart type des valeurs retardées. \n",
    "\n",
    "titre : Visualisation des variables cycliques \n",
    "\n",
    "Les variables temporelles tels que le mois et le trimestre sont cycliques. Par exemple, le mois oscille entre 1 et 12 pour chaque année. Alors que la différence entre chaque mois augmente de 1 au cours de l'année, entre deux ans, la caractéristique du mois passe de 12 (décembre) à 1 (janvier). Il en résulte une différence de -11, ce qui peut dérouter beaucoup de modèles.\n",
    "\n",
    "\n",
    "Pour résoudre ce problème, on a effectué une transformation par la fonction cosinus après avoir normalisé les variables entre 0 et 2π, ce qui correspond à un cycle de cosinus, cette solution ne régle pas le problème complétement car deux valeurs différentes peuvent avoir la même image par la fonction cosinus. La meilleure façon de résoudre ce nouveau problème était d'ajouter une autre information cyclique pour distinguer deux temps avec des valeurs de cosinus identiques, il s'agit de la fonction sinus. Nous pourrions le considérer comme un système de coordonnées à deux axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = sales_s_p64.copy()\n",
    "demo['year'] = demo.index.year\n",
    "demo['month'] = demo.index.month\n",
    "demo['quarter'] = demo.index.quarter\n",
    "demo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 3))\n",
    "sns.lineplot(x=demo.index, y=demo['quarter'], color='dodgerblue')\n",
    "ax.set_xlim([\"2014-07-31\", \"2019-09-30\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 3))\n",
    "sns.lineplot(x=demo.index, y=demo['month'], color='dodgerblue')\n",
    "ax.set_xlim([\"2014-07-31\", \"2019-09-30\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_in_year = 12\n",
    "demo['month_sin'] = np.sin(2*np.pi*demo['month']/month_in_year)\n",
    "demo['month_cos'] = np.cos(2*np.pi*demo['month']/month_in_year)\n",
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "sns.scatterplot(x=demo.month_sin, y=demo.month_cos, color='dodgerblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_quarter =4\n",
    "demo['quarter_sin'] = np.sin(2*np.pi*demo['quarter']/month_quarter)\n",
    "demo['quarter_cos'] = np.cos(2*np.pi*demo['quarter']/month_quarter)\n",
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "sns.scatterplot(x=demo.quarter_sin, y=demo.quarter_cos, color='dodgerblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme indiqué précédemment, on a utilisé les variables de la série décalées pour beneficier de l'autocorrélation existante dans la série. Pour prendre en compte l'effet de la saisonnalité, on a considéré 12 retards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 13):\n",
    "    demo[f\"lag{i}\"] = demo.P_64.shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo2 = demo.copy()\n",
    "demo2[\"Date\"] = demo[['P_64']].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a généré les moyennes mobiles et ecart-types mobiles (trimestrielle, semestrielle et annuelle) des valeurs décalées afin de détecter la tendance globale des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo2.reset_index(drop=True, inplace=True)\n",
    "window1 = 3\n",
    "window2 = 6\n",
    "window3 = 12\n",
    "\n",
    "df_rolled_3d = demo2[[\"P_64\"]].rolling(window=window1, min_periods=0)\n",
    "df_rolled_7d = demo2[[\"P_64\"]].rolling(window=window2, min_periods=0)\n",
    "df_rolled_30d = demo2[[\"P_64\"]].rolling(window=window3, min_periods=0)\n",
    "\n",
    "df_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\n",
    "df_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\n",
    "df_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n",
    "\n",
    "df_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\n",
    "df_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\n",
    "df_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n",
    "\n",
    "demo2[f\"{'P_64'}_mean_lag{window1}\"] = df_mean_3d[\"P_64\"]\n",
    "demo2[f\"{'P_64'}_mean_lag{window2}\"] = df_mean_7d[\"P_64\"]\n",
    "demo2[f\"{'P_64'}_mean_lag{window3}\"] = df_mean_30d[\"P_64\"]\n",
    "\n",
    "demo2[f\"{'P_64'}_std_lag{window1}\"] = df_std_3d[\"P_64\"]\n",
    "demo2[f\"{'P_64'}_std_lag{window2}\"] = df_std_7d[\"P_64\"]\n",
    "demo2[f\"{'P_64'}_std_lag{window3}\"] = df_std_30d[\"P_64\"]\n",
    "\n",
    "demo2.fillna(demo2.mean(), inplace=True)\n",
    "\n",
    "demo2.set_index(\"Date\", drop=False, inplace=True)\n",
    "demo2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la validation des modèles on a divisé le jeu de données en deux parties : la premiere partie pour l'entrainement du modèle (train set) et le reste (la derniere année) est reservé pour l'évaluation.\n",
    "\n",
    "On a ajusté un modèle de regression linéaire multiple sur l'ensemble des variables et on a obtenu une erreur (MAPE) de 5.29% sur le test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_serie(sales_s_p64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = prepare_data(sales_s_p64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "plotModelResults([[\"Linear Regression\", lr]], X_train=X_train, X_test=X_test, \\\n",
    "                 y_train=y_train, y_test=y_test, plot_intervals=True)\n",
    "plotCoefficients(lr, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier modèle présente des défauts\n",
    "\n",
    "La régression linéaire a donné des résultats assez biens. Cependant ce modèle simple présente des défauts,certaines variables explicatives sont fortement corrélées et toutes les variables ne sont pas également saines,certaines peuvent entraîner un surapprentissage tandis que d'autres doivent être supprimées.\n",
    "\n",
    "- le modèle n'arrive pas à utiliser toute l'information disponible aux varibales explicatives ??\n",
    "\n",
    "D'autre part, puisque on a différentes échelles dans les variables, celles ci doivent etre transformées en une même échelle pour explorer l'importance des caractéristiques et, plus tard, la régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = sns.clustermap(X_train_scaled.corr(), figsize=(10, 10), cbar_pos=(.1, .1, .03, .65))\n",
    "cg.ax_row_dendrogram.set_visible(False)\n",
    "cg.ax_col_dendrogram.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une meilleure optimisation des variables explicatives, on a  appliqué la régularisation. Deux des modèles de régression avec régularisation les plus populaires sont les régressions Ridge et Lasso. Ils ajoutent tous deux des contraintes supplémentaires à notre fonction de perte.\n",
    "\n",
    "Dans le cas de la régression Ridge, ces contraintes sont la somme des carrés des coefficients multipliée par le coefficient de régularisation. Plus le coefficient d'une caractéristique est grand, plus notre perte sera importante. Par conséquent, nous essaierons d'optimiser le modèle tout en gardant les coefficients assez bas.\n",
    "\n",
    "À la suite de cette régularisation L2, nous aurons un biais plus élevé et une variance plus faible, donc le modèle se généralisera mieux.\n",
    "\n",
    "Le deuxième modèle de régression, la régression Lasso, ajoute à la fonction de perte, non pas des carrés, mais des valeurs absolues des coefficients. Par conséquent, au cours du processus d'optimisation, les coefficients des caractéristiques sans importance peuvent devenir des zéros, ce qui permet une sélection automatisée des caractéristiques. Ce type de régularisation est appelé L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(cv=tscv)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "plotModelResults([[\"ridge\", ridge]], \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled,\n",
    "                 y_train=y_train,\n",
    "                 y_test=y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(ridge, X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure ci-dessous représente les prévisions pour chaque produit par le modèle qui minimise l'erreur (MAPE) parmis les deux modèles de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(cv=tscv)\n",
    "lasso = LassoCV(cv=tscv)\n",
    "auto_features_forecast(sales_s, [[\"Lasso\", lasso], [\"Ridge\", ridge]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, plot_importance, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {'colsample_bytree': 0.7855436372368847,\n",
    " 'gamma': 0.2715026337470021,\n",
    " 'max_depth': 11,\n",
    " 'min_child_weight': 1.0,\n",
    " 'reg_lambda': 0.358974472027333,\n",
    " 'subsample': 0.8129759325416941}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = XGBRegressor(n_estimators =1000, **best)\n",
    "reg.fit(X_train_scaled, y_train,\n",
    "        eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_pred = y_test.index\n",
    "pred_series = pd.Series(prediction, index=index_of_pred)\n",
    "plt.plot(pred_series, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "plt.plot(pd.concat([y_train, y_test]), label=\"actual\", linewidth=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotModelResults([[\"xgboost\", reg]], \n",
    "                         X_train=X_train_scaled, \n",
    "                         X_test=X_test_scaled, \n",
    "                         y_train=y_train,\n",
    "                         y_test=y_test,\n",
    "                         plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_importance(reg, height=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auto_features_forecast(sales_s, [[\"tuned_xgboost\", 0], [\"default_xgboost\", XGBRegressor(n_estimators =1000)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_features_forecast(sales_s, [[\"tuned_xgboost\", 0], [\"default_xgboost\", XGBRegressor(n_estimators =1000)],\\\n",
    "                                 [\"Lasso\", lasso], [\"Ridge\", ridge]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
